---
layout:     post
title:      敏感信息扫描工具使用对比
subtitle:   dirsearch
date:       2018-01-08
author:     zeroisone
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - Web安全
    - tools
---



# 敏感信息扫描工具使用对比
## 0x00 前言
  渗透测试过程中，经常需要对网站进行全面的信息收集，扫描网站是否存在敏感信息文件如phpinfo、日志文件泄漏会极大的方便我们进一步的渗透测试，特别是对于一些403网站，通过爆破web目录和文件的方式，对于测试网站有极大的帮助。
  近期在使用敏感信息扫描工具BBScan、dirsearch、opendoor等批量扫描网站时遇到几个问题，在此记录下，并对如何降低扫描工具的误报率做了初步分析。

## 0x01 批量扫描时遇到的问题
### BBScan
  BBScan是一个迷你的信息泄漏批量扫描脚本。规则字典非常小，但尽量保证准确和可利用性。 
 *特性*
* 小字典
* 可以自动扫描 Target/Mask 网络中所有的主机 
* 误报率小

在测试中发现有许多域名被BBScan判断为无效域名，但实际上这些网站是完全能够ping通并访问的。
![](/img/in-post/dirscan/图片1.png)
BBScan中引入了dnspython模块，用于域名的DNS解析。判断域名无效的函数如下。
![](/img/in-post/dirscan/图片2.png)
进一步测试发现是由dns解析超时导致的。

![](/img/in-post/dirscan/图片3.png)

考虑到目前很多网站采用了cdn的形式，BBScan的IP扫描准确率不高，我去掉了domain_lookup函数。
![](/img/in-post/dirscan/图片4.png)
![](/img/in-post/dirscan/图片5.png)

### dirsearch
  dirsearch基于python3，拥有可保持连接、启发式检测无效的网页、请求延迟、递归的暴力扫描等特点
  批量扫描时，程序出现异常中断。问题出现在thirdparty/sqlmap/DynamicContentParser.py文件removeDynamicContent函数，在使用re.sub正则表达式处理时出现问题，详情：”TypeError: a bytes-like object is required, not 'str' “。我对处理正则表达式时添加了异常处理，以便程序出现异常情况时能继续完成批量扫描，如下所示。
![](/img/in-post/dirscan/图片6.png)

Dirsearch的误报率较高，返回字节数相同且有多个敏感文件的都是为误报，可以根据这个判断条件降低误报率。
![](/img/in-post/dirscan/图片7.png)

## 0x02 敏感文件扫描思路
很多web扫描器的开始都是对url进行网站目录和文件扫描，然后再结合爬虫一起爬一下。

目前敏感信息扫描工具的一般逻辑是:
```
1.取一个不可能存在的url，判断是不是存在404的状态码。
  存在的404话，下面的判断只要根据状态码来判断是否存在就好了。
  不存在的话走下面2的逻辑。

2.获取该不存在url的相应内容，作为一个404不存在的页面标示，接下去的扫描如果获取到页面不和这个404表示一样的就认为是200的存在页面。
这个思路在大部分情况下面用。但是有些站点，你会发现扫到一堆200的误报。分析为什么：
```
在逻辑1中，404的判断取一个不存在的url是不够的，你需要取多个，因为有的站点xxxx.php是404，而xxxx.jsp就变成200了。

在逻辑2中，有种情况是这个404的页面是会不断变化的，比如淘宝的404页面中，下面会出现不同的链接商品推荐。这样就会产生一大堆的误报。

### BBScan
BBScan提供了“-nn, --no-check404    No HTTP 404 existence check”不检测网站404页面选项，默认是会先检测404页面的，正如上面所说，访问一个不存在的链接“BBScan-404-existence-check”。
![](/img/in-post/dirscan/图片8.png)
不检测404页面时，通过返回状态码404作为404页面判断。
![](/img/in-post/dirscan/图片9.png)

### awvs的网站目录和文件扫描思路

awvs的目录和文件扫描姿势:（Backup_File.script，Sensitive_Files.script, Possible_Sensitive_Directories ,Possible_Sensitive_Files ）

```
第一）在Backup_File和Sensitive_Files中看到能用正则匹配的，先用规则来匹配，这个比较准确，误报低。

第二) Backup_File中我们发现，awvs的再解决逻辑2中出现的问题时候，用了一个小的tip:在发现页面内容和404标示不一样的时候，再去取了一个不存在的url2获取新的404标示，然后判断两个标示是不是一样，一样的话说明这个200扫描没用问题，从而去掉这个误报。

第三) 在Possible_Sensitive_Directories ,Possible_Sensitive_Files中，我们发现awvs去掉了逻辑2。只对存在404的url进行目录和文件扫描。而目录扫描和文件扫描的逻辑不一样的，我们发现当一个文件存在时候返回时200，但是当一个目录存在的时候确是302跳转，我们需要匹配http头的Location进行判断。开源扫描工具大多数使用了python的requests库，这个库默认是会进行url跟随跳转的。所以他们没有必要区分扫描目录和文件扫描。如果你把requests中设置了allow_redirects=False，那你就要去自己匹配http头的Location关键字。
```

## 0x03 降低误报率的一些方法
* 对多个返回数据包大小一样的并超过一定数量，舍弃这些数据；
* 当命中规则时会再一次进行返回包长度的检验。


## 0x04参考

https://paper.seebug.org/461/ 对AWVS一次简单分析


